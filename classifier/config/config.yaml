server:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  threads: 4
  timeout: 120
  keep_alive: 10
  max_requests: 1000
  max_requests_jitter: 50
  worker_connections: 1000
  backlog: 2048
  log_level: "INFO"
  preload_app: true

model:
  save_dir: "/app/models"
  instances: 1
  max_length: 512
  batch_size: 32
  name: "distilbert-base-uncased"

# Dynamic weights based on task importance:
# Critical tasks (math, coding, reasoning): quality=0.9, cost=0.1
# Medium tasks (translation, writing): quality=0.6, cost=0.4  
# Casual tasks (conversation, roleplay): quality=0.2, cost=0.8
weights:
  quality: 0.7
  cost: 0.3

model_scores:
  models:
    llama3.2:
      cost_per_request: 0.001
      conversation: 0.88
      classification: 0.85
      roleplay: 0.87
      data_analysis: 0.83
      translation: 0.89
      problem_solving: 0.86
      reasoning: 0.87
      code_generation: 0.88
      writing: 0.86
      summarization: 0.87
      math: 0.85
      creative: 0.86
      research: 0.84
      extraction: 0.85
      
    gemini-2.0-flash:
      cost_per_request: 0.01
      conversation: 0.95
      classification: 0.90
      roleplay: 0.95
      data_analysis: 0.92
      translation: 0.88
      problem_solving: 0.95
      reasoning: 0.96
      code_generation: 0.94
      writing: 0.93
      summarization: 0.91
      math: 0.94
      creative: 0.92
      research: 0.93
      extraction: 0.90
