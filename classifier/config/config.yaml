model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  save_dir: "classifier/models"
  embedding_dim: 768
  cache_embeddings: true
  max_cache_size: 10000

categories:
  - conversation
  - classification
  - roleplay
  - data_analysis
  - translation
  - problem_solving
  - reasoning
  - code_generation
  - writing
  - summarization
  - math
  - creative
  - research
  - extraction

# Dynamic weights based on task importance:
# Critical tasks (math, coding, reasoning): quality=0.9, cost=0.1
# Medium tasks (translation, writing): quality=0.6, cost=0.4  
# Casual tasks (conversation, roleplay): quality=0.2, cost=0.8
weights:
  quality: 0.7  # Default fallback (not used with dynamic weighting)
  cost: 0.3     # Default fallback (not used with dynamic weighting)

model_scores:
  models:
    llama3.2:
      cost_per_request: 0.001
      conversation: 0.88
      classification: 0.85
      roleplay: 0.87
      data_analysis: 0.83
      translation: 0.89
      problem_solving: 0.86
      reasoning: 0.87
      code_generation: 0.88
      writing: 0.86
      summarization: 0.87
      math: 0.85
      creative: 0.86
      research: 0.84
      extraction: 0.85
      
    gemini-2.0-flash:
      cost_per_request: 0.01
      conversation: 0.95
      classification: 0.90
      roleplay: 0.95
      data_analysis: 0.92
      translation: 0.88
      problem_solving: 0.95
      reasoning: 0.96
      code_generation: 0.94
      writing: 0.93
      summarization: 0.91
      math: 0.94
      creative: 0.92
      research: 0.93
      extraction: 0.90

server:
  host: "0.0.0.0"
  port: 8000
  workers: 8
  threads: 4
  log_level: "INFO"
  timeout: 120
  keep_alive: 10
  max_requests: 1000
  max_requests_jitter: 50
  preload_app: true
  worker_connections: 1000
  backlog: 2048 